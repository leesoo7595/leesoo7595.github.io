---
layout: post
title: "[ML] Reducing Loss"
date: 2018-10-06
tags: [ML]
comments: true
---

참고 :
- https://developers.google.com/machine-learning/crash-course/reducing-loss/an-iterative-approach
- https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent
- https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate

# 손실 줄이기

## 반복 방식

**머신러닝 모델이 반복을 통해 어떻게 손실을 줄일까?**

처음에는 임의의 지점에서 시작하여 손실값을 알아낸다. 그런 후에 다른 값을 추정하여 손실값을 다시 확인한다. 이런 식으로 점점 손실값을 줄여나가는 것이다.
**여기서 최적의 모델을 가능한 한 가장 효율적으로 찾아내는 것이 중요하다.**

모델은, 하나 이상의 특성을 입력하여 하나의 예측을 출력하게 된다. <br>
임의의 모델 함수 -> `y' = b + wx'` <br>

`b`와 `w`의 초기값은 무엇으로 설정해야하는가? <br>
선형 회귀 문제에서는 초기값이 중요하지 않으므로 임의의 값을 정해도 상관없다. <br>
- b = 0
- w = 0
그리고 최초 특성 값이 10이라고 가정했을 때, 예측 함수에 입력하면 다음과 같이 출력된다. 이 과정은 `손실 계산 과정`에 해당된다.

```
y' = 0 + 0(10)
 y' = 0
```

위의 `손실 계산 과정`은 이 모델에서 사용할 `손실 함수`이다. 제곱 손실 함수를 사용한다고 가정했을 때, `손실 함수`는 두 개의 입력 값을 갖는다.

- y' : 특성 x에 대한 모델의 예측 값이다.
- y : 특성 x에 대한 올바른 라벨이다.

그렇게 `손실 계산 과정`을 통해 `손실 함수`를 구한 뒤에는 `매개변수 업데이트 계산 과정`을 하게 된다. 이 지점에서 머신러닝 시스템은 `손실 함수`의 값을 검토하여 `b`와 `w`의 새로운 값을 생성한다.

 이런 식으로 머신러닝 시스템이 이러한 모든 특성을 모든 라벨과 대조하여 재평가해서 `손실 함수`의 새로운 값을 생성하여 새 매개변수 값을 출력한다고 하면, 그런 후 **알고리즘이 손실 값이 가장 낮은 모델 매개변수를 발견할 때까지** 반복 학습을 하게된다. 일반적으로 전체 손실이 변하지 않거나 매우 느리게 변할 때까지 계속 반복하게 된다.

**이때 모델이 수렴했다고 이야기한다.**

## 경사하강법

`반복 방식 다이어그램`(위의 과정)의 `매개변수 업데이트 계산 과정`의 알고리즘을 더 구체적으로 예를 들어보겠다. <br>

`w`의 가능한 모든 값에 대해 손실을 계산할 시간과 컴퓨팅 자료가 있다고 가정한다. 지금까지 봐왔던 회귀 문제에서 손실과 `w`를 대응한 도표는 항상 `볼록 함수 모양`을 할 것이다. 즉 항상 그릇 모양으로 나타난다.

`볼록 함수`일 때, 이는 기울기가 정확히 `0`인 지점에서 최소값이 하나만 존재하게 된다. 이 최소값에서 `손실 함수가 수렴`한다. <br>

여기서 전체 데이터 세트에 대해 예상할 수 있는 모든 `w` 값의 `손실 함수`를 계산하는 것은 `수렴 지점`을 찾는데 비효율적인 방법이다. 여기서 `머신러닝`에서 널리 사용하는 **경사하강법** 이 있다.

**경사하강법**
1. `w`에 대한 시작 값(시작점)을 선택한다. (시작점은 중요하지 않으므로 많은 알고리즘에서는 `w`을 0이나 임의의 값을 선택한다.)
2. 시작점에서 `손실 곡선의 기울기`를 계산한다. `기울기`는 `편미분의 벡터`로, 어느 방향이 **더 정확한지** 혹은 **부정확한지** 알려주게 된다. `단일 가중치`에 대한 `손실의 기울기`는 미분 값과 같다.

- 기울기는 항상 `손실 함수값`이 가장 크게 증가하는 방향을 향한다. `경사하강법 알고리즘`은 최대한 빨리 손실을 줄이기 위해 `기울기의 반대 방향`으로 이동한다.

3. `손실 함수 곡선`의 다음 지점을 결정하기 위해 `경사하강법 알고리즘`은 기울기의 크기 일부를 시작점에서 더한다.
4. 이 과정을 반복하여 `최소값`에 점점 접근하게 된다.

## 학습률

`기울기 벡터`는 `방향`과 `크기`를 모두 갖는다. `경사하강법 알고리즘`은 기울기에 `학습률` 또는 `보폭`이라 불리는 `스칼라`를 곱하여 다음 지점을 결정한다.
 예를 들면, `기울기`가 `2.5`이고 `학습률`이 `0.01`이면 `경사하강법 알고리즘`은 이전 지점으로부터 `0.025` 떨어진 지점을 다음 지점으로 결정하게 된다.

 **초매개변수** 는 프로그래머가 `머신러닝 알고리즘`에서 조정하는 값이다. 대부분의 `머신러닝 프로그래머`는 `학습률`을 `미세 조정`하는데 상당한 시간을 소비한다. <br>
 - `학습률`을 너무 작게 설정하면 `학습 시간`이 매우 오래 걸릴 것이다.
 - `학습률`을 너무 크게 설정하면 `양자역학 실험을 잘못한 것처럼` 다음 지점이 `곡선의 최저점`을 `이탈`할 우려가 있다.

 모든 회귀 문제에는 `골디락스 학습률`이 있다. `골디락스 값`은 `손실 함수`가 얼마나 평탄한지 여부와 관련이 있다. <br>
 `손실 함수의 기울기`가 작다면 더 큰 `학습률`을 시도해볼 수 있다. 그렇게 한다면 `작은 기울기`를 보완하고 `더 큰 보폭`을 만들어 낼 수 있다.

## 확률적 경사하강법

`경사하강법`에서 `배치`는 단일 반복에서 기울기를 계산하는 데 사용하는 예의 총 개수이다.
 지금까지 `배치`가 `전체 데이터 세트`라고 가정했지만, 여기서 `배치`는 어떤 규모인지에 따라 `데이터 세트`에 어마어마한 예측값이 포함되는 경우가 많다.
 즉 `배치`가 거대해질 수 있다. `배치`가 너무 커지면 단일 반복으로도 계산하는 데 오랜 시간이 걸릴 수 있다.

 **배치가 거대해졌을 경우, 어떻게 하면 좀 더 효율적인 시간과 적은 계산으로 적절한 기울기를 얻어낼 수 있을까?**

 `데이터 세트`에서 예를 무작위로 선택하게 된다면 훨신 적은 `데이터 세트`로 중요한 `평균값`을 추정할 수 있다.
 `확률적 경사하강법(SGD)`은 이런 사고를 더욱 확장한 것으로, 반복당 하나의 예(배치 크기 1)만을 사용한다. '확률적(Stochastic)'이란 단어는 각 배치를 포함하는 하나의 예가 무작위로 선택된다는 것을 의미한다.
 반복이 충분하면 `SGD`가 효과는 있지만 노이즈가 매우 심하다.

 `미니 배치 확률적 경사하강법(미니 배치 SGD)`는 전체 배치 반복과 SGD 간의 절충안이다. <br>
 미니 배치는 일반적으로 무작위로 선택한 10개에서 1,000개 사이의 예로 구성된다.
 미니 배치 SGD는 SGD의 노이즈를 줄이면서도 전체 배치보다는 더 효율적이다.
